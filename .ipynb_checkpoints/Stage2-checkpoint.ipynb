{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y9bRjiLQ1B-N"
   },
   "outputs": [],
   "source": [
    "# connect Google drive\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CkmVghaX1n9D"
   },
   "outputs": [],
   "source": [
    "!sudo update-alternatives --config python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MWRtgpLi07uQ"
   },
   "outputs": [],
   "source": [
    "# download packages\n",
    "!pip install transformers\n",
    "# !pip install torch\n",
    "!pip install pandas\n",
    "!pip install datasets\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sXL1coWe38F2"
   },
   "outputs": [],
   "source": [
    "# create working dir\n",
    "!mkdir ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FzY2Xv5X0QTI"
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import transformers\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "import torch\n",
    "import sklearn\n",
    "# import torch.utils.data as Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EucsivadQQWs"
   },
   "outputs": [],
   "source": [
    "cuda_available = torch.cuda.is_available()\n",
    "print(\"Using CUDA:\", cuda_available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m0KvNT5QNc4V"
   },
   "outputs": [],
   "source": [
    "# regression\n",
    "class JointScaler():\n",
    "\tdef __init__(self):\n",
    "\t\tself.means = None\n",
    "\t\tself.stddev = None\n",
    "\n",
    "\tdef fit_transform(self, data):\n",
    "\t\tself.means = np.mean(data, axis=0)\n",
    "\t\tcentereddata = data - self.means\n",
    "\t\tself.stddev = np.std(centereddata)\n",
    "\t\treturn centereddata / self.stddev\n",
    "\n",
    "\tdef transform(self, data):\n",
    "\t\treturn (data - self.means) / self.stddev\n",
    "\n",
    "\tdef inverse_transform(self, data):\n",
    "\t\treturn (data * self.stddev) + self.means\n",
    "\n",
    "class YNormalizer():\n",
    "\tdef __init__(self, settings):\n",
    "\t\tprint(\"  Output normalizer settings:\", settings)\n",
    "\t\tif settings == \"indscale\":\n",
    "\t\t\tself.scale = True\n",
    "\t\t\tself.scaler = sklearn.preprocessing.StandardScaler()\n",
    "\t\telif settings == \"jointscale\":\n",
    "\t\t\tself.scale = True\n",
    "\t\t\tself.scaler = JointScaler()\n",
    "\t\telse:\n",
    "\t\t\tself.scale = False\n",
    "\t\t\tself.scaler = None\n",
    "\n",
    "\tdef fit_transform(self, data):\n",
    "\t\tif self.scale:\n",
    "\t\t\tdata = self.scaler.fit_transform(data)\n",
    "\t\treturn data\n",
    "\n",
    "\tdef transform(self, data):\n",
    "\t\tif self.scale:\n",
    "\t\t\tdata = self.scaler.transform(data)\n",
    "\t\treturn data\n",
    "\n",
    "\tdef inverse_transform(self, data):\n",
    "\t\tif self.scale:\n",
    "\t\t\tdata = self.scaler.inverse_transform(data)\n",
    "\t\treturn data\n",
    "\n",
    "# define globally to use it in custom loss function\n",
    "normalizer = None\n",
    "\n",
    "R = 6371\n",
    "def evaluate(c1, c2, scale_km=True):\n",
    "\td = np.radians(c2-c1)\n",
    "\ta = np.sin(d[:,0]/2) * np.sin(d[:,0]/2) + np.cos(np.radians(c1[:,0])) * np.cos(np.radians(c2[:,0])) * np.sin(d[:,1]/2) * np.sin(d[:,1]/2)\n",
    "\td = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "\tif scale_km:\n",
    "\t\treturn R * d\n",
    "\telse:\n",
    "\t\treturn d\n",
    "\n",
    "def median_dist(a, b):\n",
    "\tglobal normalizer\n",
    "\ta_tr = normalizer.inverse_transform(a)\n",
    "\tb_tr = normalizer.inverse_transform(b)\n",
    "\td = evaluate(a_tr, b_tr)\n",
    "\treturn np.median(d)\n",
    "\n",
    "def mean_dist(a, b):\n",
    "\tglobal normalizer\n",
    "\ta_tr = normalizer.inverse_transform(a)\n",
    "\tb_tr = normalizer.inverse_transform(b)\n",
    "\td = evaluate(a_tr, b_tr)\n",
    "\treturn np.mean(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GFsq85bOtG2W"
   },
   "outputs": [],
   "source": [
    "# normalize lables\n",
    "def get_normalizer(args):\n",
    "  global normalizer\n",
    "  # normalizer = YNormalizer(\"jointscale\")\n",
    "  normalizer = YNormalizer(args)\n",
    "  # lables_sc = normalizer.fit_transform(lables)\n",
    "  # return lables_sc\n",
    "  return normalizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3S4bjqUxYM7F"
   },
   "outputs": [],
   "source": [
    "# pd data pre load\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class DatasetLoader():\n",
    "\tdef __init__(self, filepath, filetype, sheet_name=None, usecols=None, output_dir=\"./data\", drop_subset_duplicates=None):\n",
    "\t\tdata = None\n",
    "\t\tif not os.path.exists(filepath):\n",
    "\t\t\traise ValueError('file does not exist!')\n",
    "\t\tif filetype == \"xlsx\":\n",
    "\t\t\ttry:\n",
    "\t\t\t\tdata = pd.read_excel(filepath, sheet_name=sheet_name, usecols=usecols)\n",
    "\t\t\texcept RuntimeError as e:\n",
    "\t\t\t\tprint(e.args)\n",
    "\t\telif filetype == \"csv\":\n",
    "\t\t\tdata = pd.read_csv(filepath)\n",
    "\t\tif data.empty:\n",
    "\t\t\traise RuntimeError('data empty!')\n",
    "\t\tif drop_subset_duplicates:\n",
    "\t\t\tdata = data.drop_duplicates(subset=drop_subset_duplicates, keep=\"first\")\n",
    "\t\tif not os.path.exists(output_dir):\n",
    "\t\t\tos.mkdir(output_dir)\n",
    "\t\tdata.to_csv(os.path.join(output_dir, \"data.csv\"), index=None)\n",
    "\n",
    "\tdef get_dataset(self, file_path=\"./data/data.csv\", objtype=\"df\", split=0.8, filter_subset=None):\n",
    "\t\tif not os.path.exists(file_path):\n",
    "\t\t\traise ValueError('file does not exist!')\n",
    "\t\tdata = None\n",
    "\t\tif objtype == \"df\":\n",
    "\t\t\tdata = pd.read_csv(file_path)\n",
    "\t\t\tif filter_subset:\n",
    "\t\t\t\tdata = data.dropna(subset=filter_subset)\n",
    "\t\telif objtype == \"ds\":\n",
    "\t\t\tfile_format = os.path.splitext(file_path)[-1]\n",
    "\t\t\tif file_format in [\"csv\", \"json\"]:\n",
    "\t\t\t\tdata = data.load_dataset(\"csv\", data_files=file_path)\n",
    "\t\t\telse:\n",
    "\t\t\t\traise ValueError('file format not supported')\n",
    "\t\t\tif filter_subset:\n",
    "\t\t\t\tdata = data.filter(lambda x: x[f] for f in filter_subset)\n",
    "\t\telse:\n",
    "\t\t\t# return list\n",
    "\t\t\tpass\n",
    "\t\treturn data\n",
    "\n",
    "file_path = \"/content/drive/MyDrive/data/RM_Wyoming_georef_training_2022-07-11.xlsx\"\n",
    "data_dir = \"/content/data\"\n",
    "\n",
    "obj_df = DatasetLoader(\n",
    "    filepath = file_path,\n",
    "    filetype = \"xlsx\",\n",
    "    sheet_name = \"RM_Woming_georef_training\",\n",
    "    usecols = [0, 8, 9, 16, 17, 18, 19, 23, 26, 27],\n",
    "    output_dir = data_dir,\n",
    "    drop_subset_duplicates = ['Country','State', 'County', 'Locality', 'GeorefLatitude', 'GeorefLongitude']\n",
    ")\n",
    "\n",
    "data_df = obj_df.get_dataset(file_path = os.path.join(data_dir, \"data.csv\"), objtype = \"df\", split = 0.8, filter_subset = [\"Locality\", 'GeorefLatitude', 'GeorefLongitude'])\n",
    "print(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SuuaBGtYiaov"
   },
   "outputs": [],
   "source": [
    "# normalize lables\n",
    "normalizer = get_normalizer(\"jointscale\")\n",
    "checkpoint=\"roberta-base\"\n",
    "\n",
    "# tokenizer process\n",
    "class MyTokenzier():\n",
    "  def __init__(self, filepath, filetype, usecols=None, filter_fc=None, checkpoint=\"roberta-base\", classifier=False):\n",
    "    self.checkpoint = checkpoint\n",
    "    self.tokenizer = AutoTokenizer.from_pretrained(self.checkpoint)\n",
    "    self.data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)   \n",
    "    self.dataset = load_dataset(filetype, data_files=filepath)\n",
    "    self.class_dics = {}\n",
    "     # clean & filter data\n",
    "    if filter_fc:\n",
    "      self.dataset = self.dataset.filter(filter_fc)\n",
    "    # convert Family‚Äù and ScientificName to class ID\n",
    "    if classifier:\n",
    "      self.dataset = self.convert_classifier_id()\n",
    "    # split data into train data and dev data\n",
    "    self.dataset = self.dataset[\"train\"].train_test_split(train_size=0.8, seed=42)\n",
    "    self.dataset[\"dev\"] = self.dataset.pop(\"test\")\n",
    "    # process labels\n",
    "    self.train_y_sc, self.dev_y_sc = self.set_labels(classifier)\n",
    "    print(\"  {} training examples with {} output features\".format(len(self.train_y_sc), self.train_y_sc.shape[1]))\n",
    "    # import pretrained model\n",
    "    self.model = AutoModelForSequenceClassification.from_pretrained(self.checkpoint, num_labels = self.train_y_sc.shape[1])\n",
    "    # print(self.dataset)\n",
    "\n",
    "  def convert_classifier_id(self):\n",
    "    def convert_dics(data):\n",
    "      family = data[\"Family\"]\n",
    "      name = data[\"ScientificName\"]\n",
    "      id = len(self.class_dics)\n",
    "      if family not in self.class_dics:\n",
    "        self.class_dics[family] = [id*100, {}]\n",
    "      family_id, family_ls = self.class_dics[family][0], self.class_dics[family][1]\n",
    "      if name not in family_ls:\n",
    "        class_id = family_id + len(family_ls) + 1\n",
    "        self.class_dics[family][1][name] = class_id\n",
    "      else:\n",
    "        class_id = self.class_dics[family][1][name]\n",
    "      return {\"class_id\": class_id }\n",
    "    # print(self.class_dics)\n",
    "    return self.dataset.map(convert_dics)\n",
    "\n",
    "  def set_labels(self, classifier=None):\n",
    "    train_y, dev_y = self.dataset[\"train\"], self.dataset[\"dev\"]\n",
    "    train_y_sc = normalizer.fit_transform(np.array(np.c_[train_y[\"GeorefLatitude\"], train_y[\"GeorefLongitude\"]], dtype=np.float32)) \n",
    "    dev_y_sc = normalizer.transform(np.array(np.c_[dev_y[\"GeorefLatitude\"], dev_y[\"GeorefLongitude\"]], dtype=np.float32)) \n",
    "    if classifier:\n",
    "      train_y_sc = np.array([np.append(x, train_y[i][\"class_id\"]) for i,x in enumerate(train_y_sc)], dtype=np.float32)\n",
    "      dev_y_sc = np.array([np.append(x, dev_y[i][\"class_id\"]) for i,x in enumerate(dev_y_sc)], dtype=np.float32)\n",
    "    return train_y_sc, dev_y_sc\n",
    "\n",
    "  def tokenization(self, cols=[\"Locality\"]):\n",
    "    train_x, dev_x = self.dataset[\"train\"], self.dataset[\"dev\"]\n",
    "    if len(cols) == 1:\n",
    "      train_df = pd.DataFrame(zip(train_x[cols[0]], self.train_y_sc))\n",
    "      train_df.columns = [cols[0], \"labels\"]\n",
    "      dev_df = pd.DataFrame(zip(dev_x[cols[0]], self.dev_y_sc))\n",
    "      dev_df.columns = [cols[0], \"labels\"]\n",
    "    else:\n",
    "      train_df = pd.DataFrame(zip(train_x[cols[0]], train_x[cols[1]], self.train_y_sc))\n",
    "      train_df.columns = [cols[0], cols[1], \"labels\"]\n",
    "      dev_df = pd.DataFrame(zip(dev_x[cols[0]], dev_x[cols[1]], self.dev_y_sc))\n",
    "      dev_df.columns = [cols[0], cols[1], \"labels\"]\n",
    "    \n",
    "    # tokenizer\n",
    "    def tokenize_function(data):\n",
    "      return self.tokenizer(data[cols[0]], truncation=True) if len(cols) == 1 else self.tokenizer(data[cols[0]], data[cols[1]], truncation=True)\n",
    "    \n",
    "    train_ds = Dataset.from_pandas(train_df)\n",
    "    train_ds = train_ds.map(tokenize_function, batched=True, remove_columns=cols)\n",
    "    dev_ds = Dataset.from_pandas(dev_df)\n",
    "    dev_ds = dev_ds.map(tokenize_function, batched=True, remove_columns=cols)\n",
    "    return train_ds, dev_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KseTIash0vhs"
   },
   "outputs": [],
   "source": [
    "# Specifiy the arguments for the trainer  \n",
    "training_args = TrainingArguments(\n",
    "    output_dir ='./results',\n",
    "    num_train_epochs = 20,\n",
    "    per_device_train_batch_size = 32,\n",
    "    per_device_eval_batch_size = 20,\n",
    "    weight_decay = 0.01,\n",
    "    learning_rate = 2e-5,\n",
    "    logging_dir = './logs',\n",
    "    metric_for_best_model = 'loss',\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    prediction_loss_only = False,\n",
    "    save_total_limit = 1,\n",
    "    load_best_model_at_end=True,\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i66laiQj0nib"
   },
   "outputs": [],
   "source": [
    "# trainer\n",
    "class RoBertaTrainer(Trainer):\n",
    "  def __init__(self, loss_fct=\"MSELoss\", *args, **kwargs):\n",
    "    super().__init__(*args, **kwargs)\n",
    "    self.loss_fct = loss_fct\n",
    "    # print(self.loss_fct)\n",
    "    # print(self.model)\n",
    "    if self.loss_fct == \"MAELoss\":\n",
    "      self.loss_fct = torch.nn.MSELoss()\n",
    "    elif loss_fct == \"MSELoss\":\n",
    "      self.loss_fct = torch.nn.L1Loss()\n",
    "    elif loss_fct == \"CrossEntropyLoss\":\n",
    "      self.loss_fct = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "  def compute_loss(self, model, inputs, return_outputs=False):\n",
    "    # implement custom logic here\n",
    "    labels = inputs.get(\"labels\")\n",
    "    # forward pass\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.get(\"logits\")\n",
    "    # compute custom loss\n",
    "    loss = self.loss_fct(logits.view(-1, model.num_labels), labels.view(-1, model.num_labels))\n",
    "    return (loss, outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kWrck3CD0HYl"
   },
   "outputs": [],
   "source": [
    "# Compute Metrics\n",
    "def compute_metrics_for_regression(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    # print(\"eval_pred: \", eval_pred)\n",
    "    # print(\"logits: \", logits)\n",
    "    median = median_dist(logits, labels)\n",
    "    mean = mean_dist(logits, labels)\n",
    "    return {\"Median(km)\": median, \"Mean(km)\": mean}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fwsUpb05f9UH"
   },
   "outputs": [],
   "source": [
    "# Experiment 1\n",
    "# [Locality] -> [lat, lon], MAE/MSE\n",
    "tokens = MyTokenzier(\"./data/data.csv\", \"csv\", checkpoint=checkpoint, filter_fc=(lambda x: x[\"Locality\"] is not None and x[\"SiteDescription\"] is not None and x[\"GeorefLatitude\"] is not None and x[\"GeorefLongitude\"] is not None))\n",
    "train_ds, dev_ds = tokens.tokenization(cols=[\"Locality\"])\n",
    "print(train_ds)\n",
    "print('train_df_data', train_ds[:10])\n",
    "print('train_df features', train_ds.features)\n",
    "\n",
    "loss_fct = \"MAELoss\"\n",
    "# loss_fct = \"MSELoss\"\n",
    "\n",
    "# Call the Trainer\n",
    "trainer = RoBertaTrainer(\n",
    "    model = tokens.model,                         \n",
    "    args = training_args,                  \n",
    "    train_dataset = train_ds,         \n",
    "    eval_dataset = dev_ds,          \n",
    "    compute_metrics = compute_metrics_for_regression,    \n",
    "    tokenizer=tokens.tokenizer,\n",
    "    data_collator=tokens.data_collator, \n",
    "    loss_fct=loss_fct\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "# Call the summary\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z-pGSOGUgbaZ"
   },
   "outputs": [],
   "source": [
    "# Experiment 2\n",
    "# [Locality, SiteDescription] -> [lat, lon], MAE/MSE\n",
    "tokens = MyTokenzier(\"./data/data.csv\", \"csv\", checkpoint=checkpoint, filter_fc=(lambda x: x[\"Locality\"] is not None and x[\"SiteDescription\"] is not None and x[\"GeorefLatitude\"] is not None and x[\"GeorefLongitude\"] is not None))\n",
    "train_ds, dev_ds = tokens.tokenization(cols=[\"Locality\", \"SiteDescription\"])\n",
    "print(train_ds)\n",
    "print('train_df_data', train_ds[:10])\n",
    "print('train_df features', train_ds.features)\n",
    "\n",
    "# loss_fct = \"MAELoss\"\n",
    "loss_fct = \"MSELoss\"\n",
    "\n",
    "# Call the Trainer\n",
    "trainer = RoBertaTrainer(\n",
    "    model = tokens.model,                         \n",
    "    args = training_args,                  \n",
    "    train_dataset = train_ds,         \n",
    "    eval_dataset = dev_ds,          \n",
    "    compute_metrics = compute_metrics_for_regression,    \n",
    "    tokenizer=tokens.tokenizer,\n",
    "    data_collator=tokens.data_collator, \n",
    "    loss_fct=loss_fct\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "# Call the summary\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s3is6oHHRqwU"
   },
   "outputs": [],
   "source": [
    "# trainer\n",
    "class RoBertaClassifierTrainer(Trainer):\n",
    "  def __init__(self, loss_fct=\"MSELoss\", classifier=\"CrossEntropyLoss\", *args, **kwargs):\n",
    "    super().__init__(*args, **kwargs)\n",
    "    self.loss_fct = loss_fct\n",
    "    # print(self.loss_fct)\n",
    "    # print(self.model)\n",
    "    if self.loss_fct == \"MAELoss\":\n",
    "      self.loss_fct = torch.nn.MSELoss()\n",
    "    elif loss_fct == \"MSELoss\":\n",
    "      self.loss_fct = torch.nn.L1Loss()\n",
    "    self.classifier = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "  def compute_loss(self, model, inputs, return_outputs=False):\n",
    "    # implement custom logic here\n",
    "    labels = inputs.get(\"labels\")\n",
    "    # forward pass\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.get(\"logits\")\n",
    "    reg_logits, classf_logits = torch.tensor_split(logits, 2, dim=1)\n",
    "    # print(reg_logits, classf_logits)\n",
    "    # print(labels)\n",
    "    reg_labels, classf_labels = torch.tensor_split(labels, 2, dim=1)\n",
    "    # compute custom loss\n",
    "    loss = self.loss_fct(reg_logits.view(-1, model.num_labels-1), reg_labels.view(-1, model.num_labels-1)) + self.classifier(classf_logits.view(-1, model.num_labels-2), classf_labels.view(-1, model.num_labels-2))\n",
    "    return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "# Compute Metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    # print(\"eval_pred: \", eval_pred)\n",
    "    # print(\"logits: \", logits)\n",
    "    reg_logits = np.array(list(map(lambda x: [x[0], x[1]], logits)), dtype=np.float32)\n",
    "    reg_labels = np.array(list(map(lambda x: [x[0], x[1]], labels)), dtype=np.float32)\n",
    "    print(reg_logits.shape, reg_labels.shape)\n",
    "    median = median_dist(reg_logits, reg_labels)\n",
    "    mean = mean_dist(reg_logits, reg_labels)\n",
    "    return {\"Median(km)\": median, \"Mean(km)\": mean}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O7MTZTgns77q"
   },
   "outputs": [],
   "source": [
    "# Experiment 3\n",
    "# [Locality, Famliy, ScientificName] -> [lat, lon], MAE+Classifier(CrossEntropyLoss) / MSE+Classifier(CrossEntropyLoss)tokens = MyTokenzier(\"./data/data.csv\", \"csv\", checkpoint=checkpoint, classifier=True, filter_fc=(lambda x: x[\"Family\"] is not None and x[\"ScientificName\"] is not None and x[\"Locality\"] is not None and x[\"SiteDescription\"] is not None and x[\"GeorefLatitude\"] is not None and x[\"GeorefLongitude\"] is not None))\n",
    "train_ds, dev_ds = tokens.tokenization(cols=[\"Locality\"])\n",
    "print(train_ds)\n",
    "print('train_df_data', train_ds[:10])\n",
    "print('train_df features', train_ds.features)\n",
    "\n",
    "# Call the Trainer\n",
    "classifier = \"CrossEntropyLoss\"\n",
    "# loss_fct = \"MAELoss\"\n",
    "loss_fct = \"MSELoss\"\n",
    "\n",
    "# MAE\n",
    "trainer = RoBertaClassifierTrainer(\n",
    "    model = tokens.model,                         \n",
    "    args = training_args,                  \n",
    "    train_dataset = train_ds,         \n",
    "    eval_dataset = dev_ds,          \n",
    "    compute_metrics = compute_metrics,    \n",
    "    tokenizer=tokens.tokenizer,\n",
    "    data_collator=tokens.data_collator, \n",
    "    loss_fct=loss_fct,\n",
    "    classifier=classifier\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "# Call the summary\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JvVsyvENgbkE"
   },
   "outputs": [],
   "source": [
    "# Experiment 4\n",
    "# [Locality, SiteDescription, Famliy, ScientificName] -> [lat, lon], MAE+Classifier(CrossEntropyLoss) / MSE+Classifier(CrossEntropyLoss)\n",
    "tokens = MyTokenzier(\"./data/data.csv\", \"csv\", checkpoint=checkpoint, classifier=True, filter_fc=(lambda x: x[\"Family\"] is not None and x[\"ScientificName\"] is not None and x[\"Locality\"] is not None and x[\"SiteDescription\"] is not None and x[\"GeorefLatitude\"] is not None and x[\"GeorefLongitude\"] is not None))\n",
    "train_ds, dev_ds = tokens.tokenization(cols=[\"Locality\", \"SiteDescription\"])\n",
    "print(train_ds)\n",
    "print('train_df_data', train_ds[:10])\n",
    "print('train_df features', train_ds.features)\n",
    "\n",
    "# Call the Trainer\n",
    "classifier = \"CrossEntropyLoss\"\n",
    "# loss_fct = \"MAELoss\"\n",
    "loss_fct = \"MSELoss\"\n",
    "\n",
    "# MAE\n",
    "trainer = RoBertaClassifierTrainer(\n",
    "    model = tokens.model,                         \n",
    "    args = training_args,                  \n",
    "    train_dataset = train_ds,         \n",
    "    eval_dataset = dev_ds,          \n",
    "    compute_metrics = compute_metrics,    \n",
    "    tokenizer=tokens.tokenizer,\n",
    "    data_collator=tokens.data_collator, \n",
    "    loss_fct=loss_fct,\n",
    "    classifier=classifier\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "# Call the summary\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d-XFdgQpQ36v"
   },
   "outputs": [],
   "source": [
    "# test & predict\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
